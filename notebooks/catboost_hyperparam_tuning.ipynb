{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit ('venv': venv)",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "6704a13a11f20111acd916d3e024303c69ba0f7542ff959e77e5fa0d802aa562"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train and test data from csv files\n",
    "file_name = 'data.csv'\n",
    "colnames = ['col1','col2','col3','col4','label']\n",
    "category_cols = ['col2','col2']\n",
    "\n",
    "LABEL = 'label'\n",
    "TASK = 'CPU'\n",
    "TYPE = 'classification' # [classification, regression, multiclass]\n",
    "SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_name, usecols=colnames)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert categorical columns to integers\n",
    "cat_dims = [data.columns.get_loc(i) for i in category_cols[:-1]] \n",
    "for header in category_cols:\n",
    "    data[header] = data[header].astype('category').cat.codes\n",
    "\n",
    "X = data.drop(LABEL, axis=1)\n",
    "Y = data[LABEL]\n",
    "\n",
    "if TYPE == 'regression':\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=SPLIT)\n",
    "    class_weights = None\n",
    "else:\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=SPLIT, stratify=Y)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(train_y), train_y)\n",
    "\n",
    "train_pool = cb.Pool(train_x, label=train_y, cat_features=category_cols)\n",
    "test_pool = cb.Pool(test_x, label=test_y, cat_features=category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE == 'regression':\n",
    "    loss = 'MAE'\n",
    "    eval = ['RMSE', 'MAPE', 'MAE', 'R2', 'MedianAbsoluteError']\n",
    "    metric = 'RMSE'\n",
    "elif TYPE == 'classification':\n",
    "    loss = 'Logloss'\n",
    "    eval = ['Logloss', 'CrossEntropy', 'Precision', 'Recall', 'F1', 'Accuracy', 'AUC']\n",
    "    metric = 'Accuracy'\n",
    "elif TYPE == 'multiclass':\n",
    "    loss = 'MultiClassOneVsAll'\n",
    "    eval = ['MultiClass', 'MultiClassOneVsAll', 'Precision', 'Recall', 'F1', 'Accuracy', 'AUC']\n",
    "    metric = 'Accuracy'\n",
    "\n",
    "params = {'depth':[3,1,2,6,4,5,7,8,9,10],\n",
    "          'iterations':[50,100,250,500,1000],\n",
    "          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3], \n",
    "          'l2_leaf_reg':[3,1,5,10,50],\n",
    "          'border_count':[32,5,10,20,50,100,200],\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = cb.CatBoost({\n",
    "            'thread_count':-1, \n",
    "            'task_type': TASK,\n",
    "            'loss_function': loss,\n",
    "            'eval_metric': metric,\n",
    "            'class_weights': class_weights\n",
    "})\n",
    "\n",
    "search_result = model.randomized_search(params, train_pool, n_iter=100, cv=3, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result['cv_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = search_result['params']\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TYPE == 'regression':\n",
    "    model = CatBoostRegressor(**{\n",
    "            'thread_count':-1, \n",
    "            'loss_function': loss,\n",
    "            'eval_metric': metric,\n",
    "    })\n",
    "elif (TYPE == 'classification') or (TYPE=='multiclass'):\n",
    "    model = CatBoostClassifier(**{\n",
    "            'thread_count':-1, \n",
    "            'loss_function': loss,\n",
    "            'eval_metric': metric,\n",
    "            'class_weights': class_weights\n",
    "    })\n",
    "\n",
    "model.set_params(**best)\n",
    "model.fit(train_pool, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.eval_metrics(test_pool, eval)\n",
    "for e in results.keys():\n",
    "    print(e, results[e][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('model.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_feature_importance(type='FeatureImportance', prettified=True).set_index('Feature Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_feature_importance(type='FeatureImportance', prettified=True).set_index('Feature Id').plot(kind='bar', figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_pool)\n",
    "if TYPE == 'regression':\n",
    "    preds = [np.round(np.max(p, 0), 2) for p in predictions] # assert prediciton > 0\n",
    "    diff = np.abs(predictions - test_y)\n",
    "\n",
    "    mean = np.mean(diff)\n",
    "    stdev = np.std(diff)\n",
    "    median = np.median(diff)\n",
    "    max_ = np.max(diff)\n",
    "    min_ = np.min(diff)\n",
    "\n",
    "    print('Error:')\n",
    "    print('mean', mean)\n",
    "    print('st dev', stdev)\n",
    "    print('median', median)\n",
    "    print('max', max_)\n",
    "    print('min', min_)\n",
    "\n",
    "    outlier = 10\n",
    "    print(f'more than {outlier} days', np.round(len(diff[diff > outlier])/len(diff)*100, 3), '%')\n",
    "    plt.hist(diff[diff < outlier], bins=outlier)\n",
    "    height = diff.value_counts()[0]\n",
    "    plt.vlines(mean, 0, height, label='mean', color='r')\n",
    "    plt.vlines(median, 0, height, label='median', color='g')\n",
    "    plt.legend()\n",
    "    plt.title('Error Distribution')\n",
    "    plt.show()\n",
    "\n",
    "elif (TYPE == 'classification') or (TYPE == 'multiclass'):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_y, predictions))\n",
    "\n",
    "    cf = {'Actual': test_y, 'Predicted': predictions}\n",
    "    cf_df = pd.DataFrame(data=cf)\n",
    "    confusion_matrix = pd.crosstab(cf_df['Actual'], cf_df['Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "            zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(confusion_matrix, annot=labels, fmt='')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ]
}