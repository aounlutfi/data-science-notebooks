{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_v = '_v0'\n",
    "fname = 'data.csv'\n",
    "bname = 'data_blind.csv'\n",
    "label_name = 'Label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[label_name])\n",
    "Y = data[label_name]\n",
    "labels = np.unique(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)\n",
    "dtrain = xgb.DMatrix(x_train,y_train)\n",
    "dtest = xgb.DMatrix(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcross_val = xgb.DMatrix(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 50\n",
    "es = 4\n",
    "metric = 'merror' #change metric\n",
    "folds = 3\n",
    "params = {\n",
    "    'eta':0.1,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 1,\n",
    "    'objective':'multi:softmax',\n",
    "    'nthread': 8,\n",
    "    'silent':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(dtrain=dcross_val,\n",
    "                    params=params, \n",
    "                    nfold=folds, \n",
    "                    num_boost_round=num_round,\n",
    "                    early_stopping_rounds=es,\n",
    "                    metrics=metric,\n",
    "                    callbacks=[xgb.callback.print_evaluation(show_stdv=False)],\n",
    "                    as_pandas=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight/2)\n",
    "    for max_depth in range(1,15)\n",
    "    for min_child_weight in range(1,20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = float(\"Inf\") ## change error throughout to metric\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))\n",
    "\n",
    "    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dcross_val,\n",
    "        num_boost_round=num_round,\n",
    "        seed=0,\n",
    "        nfold=folds,\n",
    "        metrics=metric,\n",
    "        early_stopping_rounds=es\n",
    "    )\n",
    "\n",
    "    mean_error = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\terror {} for {} rounds\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, ERROR: {}\".format(best_params[0], best_params[1], min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = best_params[0]\n",
    "params['min_child_weight'] = best_params[1]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(3,10)]\n",
    "    for colsample in [i/10. for i in range(3,10)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = float(\"Inf\") ## change error throughout to metric\n",
    "best_params = None\n",
    "\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dcross_val,\n",
    "        num_boost_round=num_round,\n",
    "        seed=0,\n",
    "        nfold=folds,\n",
    "        metrics=metric,\n",
    "        early_stopping_rounds=es\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_error = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\terror {} for {} rounds\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = (subsample,colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, ERROR: {}\".format(best_params[0], best_params[1], min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = best_params[0]\n",
    "params['colsample_bytree'] = best_params[1]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_error = float(\"Inf\") ## change error throughout to metric\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "\n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "\n",
    "    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dcross_val,\n",
    "        num_boost_round=num_round,\n",
    "        seed=0,\n",
    "        nfold=folds,\n",
    "        metrics=metric,\n",
    "        early_stopping_rounds=es\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_error = cv_results['test-merror-mean'].min()\n",
    "    boost_rounds = cv_results['test-merror-mean'].argmin()\n",
    "    print(\"\\terror {} for {} rounds\\n\".format(mean_error, boost_rounds))\n",
    "    if mean_error < min_error:\n",
    "        min_error = mean_error\n",
    "        best_params = eta\n",
    "\n",
    "print(\"Best params: {}, ERROR: {}\".format(best_params, min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = best_params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=es\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = model.best_iteration + 1\n",
    "\n",
    "best_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_round,\n",
    "    evals=[(dtest, \"Test\")],\n",
    "    early_stopping_rounds=es\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save_model(\"xgb\" + _v +\".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10, 10))\n",
    "xgb.plot_importance(best_model, ax =ax, title='Feature importance (Weight)', importance_type='weight', show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10, 10))\n",
    "xgb.plot_importance(best_model, ax =ax, title='Feature importance (Gain)', importance_type='gain', show_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.Booster({'nthread':8})\n",
    "model.load_model(\"xgb\" + _v + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_res, y_test)\n",
    "report = classification_report(y_res, y_test, digits=3, target_names=labels)\n",
    "cf = confusion_matrix(y_res, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)\n",
    "print(\"Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf, xticklabels=labels yticklabels=labels, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind = pd.read_csv(bname)\n",
    "x_blind = blind.drop(columns=[label_name])\n",
    "y_blind = blind[label_name]\n",
    "dblind = xgb.DMatrix(x_blind, y_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.Booster({'nthread':8})\n",
    "model.load_model(\"xgb\" + _v + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(dblind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_b, y_blind)\n",
    "report = classification_report(y_b, y_blind, digits=3, target_names=labels)\n",
    "cf = confusion_matrix(y_b, y_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)\n",
    "print(\"Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
